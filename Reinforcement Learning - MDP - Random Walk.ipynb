{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is in an horizontal world and can move left or right with probability .5.\n",
    "The reward is 0 for all transitions except for the transition to the right terminal state.\n",
    "\n",
    "We eventually introduce randomness in the transitions by giving probability .1 to stay in the current state, probability 0.1 to move 2 steps at once and probability .8 to move to the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "def move_left(current_state):\n",
    "    return current_index-1\n",
    "    \n",
    "def move_right(current_state):\n",
    "    return current_index+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2016,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(n_states, n_actions):\n",
    "\n",
    "    P = np.zeros((n_actions,n_states,n_states))\n",
    "    for i in range(1,n_states-1):\n",
    "        # move left :\n",
    "        P[0,i,i] = 0. # 0.1\n",
    "        if i-1>=0:\n",
    "            P[0,i,i-1] = 1. #0.8\n",
    "        if i-2>=0:\n",
    "            P[0,i,i-2] = 0. #0.1\n",
    "\n",
    "        # move right\n",
    "        P[1,i,i] = 0. #0.1\n",
    "        if i+1<n_states:\n",
    "            P[1,i,i+1] = 1. #0.8\n",
    "        if i+2<n_states:\n",
    "            P[1,i,i+2] = 0. #0.1\n",
    "            \n",
    "    # Terminal states\n",
    "    P[:,0,0]=1.\n",
    "    P[:,n_states-1,n_states-1]=1.\n",
    "    \n",
    "    return P\n",
    "\n",
    "def reward_matrix(n_states, n_actions):\n",
    "    R = -1*np.ones((n_actions, n_states))\n",
    "    R[0,1] = np.iinfo(np.int).min\n",
    "    R[1,n_states-2] = 10\n",
    "    R[:,0] = 0.\n",
    "    R[:,n_states-1] = 0.\n",
    "    return R\n",
    "    \n",
    "def make_random_policy(n_states, n_actions): \n",
    "    random_policy = np.ones((n_actions, n_states))/n_actions\n",
    "    # Terminal states\n",
    "    random_policy[:,0]=0.\n",
    "    random_policy[:,n_states-1]=0.\n",
    "    return random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2017,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 19\n",
    "states = list(range(n_states))\n",
    "actions = [move_left, move_right]\n",
    "n_actions = len(actions)\n",
    "P = transition_matrix(n_states, n_actions)\n",
    "R = reward_matrix(n_states, n_actions)\n",
    "random_policy = make_random_policy(n_states, n_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a policy $\\pi$ we want to evaluate the value function $v_{\\pi}$. To do so we are going to use the Bellman expectation equation in an iterative manner to improve our value function :\n",
    "$$ v_{k+1} = \\sum_{a∈A}\\pi(a/s)(R^a_s +\\gamma \\sum_{s'∈S}P^a_{ss'}v_{k}(s')) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2021,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_reward_process(R, P, policy):\n",
    "    n_actions = P.shape[0]\n",
    "    n_states = P.shape[1]\n",
    "    P_ = np.zeros_like(P[0])\n",
    "    R_ = np.sum(R*policy,axis=0)\n",
    "    for s in range(n_states):\n",
    "        P_[s,:] = policy[:,s] @ P[:,s,:]\n",
    "    return R_, P_\n",
    "\n",
    "def mdp_policy_evaluation(R, P, policy, epsilon= 10**-4, n_iter_max=2000, gamma=1, verbose=False):\n",
    "    n_states = P.shape[1]\n",
    "    R_, P_ = markov_reward_process(R, P, policy)\n",
    "    V_k = np.zeros(n_states)\n",
    "    delta = epsilon+1\n",
    "    n_iter=1\n",
    "    while delta>epsilon and n_iter<=n_iter_max:\n",
    "        V_k_1 = R_ + gamma * P_ @ V_k\n",
    "        delta = max(epsilon, np.max(np.abs(V_k_1-V_k)))\n",
    "        V_k = V_k_1\n",
    "        n_iter+=1\n",
    "    if verbose:\n",
    "        if delta>epsilon:\n",
    "            print(\"Warning : Policy evaluation did not converge after %d steps\"%(n_iter-1))\n",
    "            print(\"delta : {}\".format(delta))\n",
    "        else:\n",
    "            print(\"Policy evaluation did converge after %d steps\"%(n_iter-1))\n",
    "    return V_k_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2023,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation did converge after 1748 steps\n"
     ]
    }
   ],
   "source": [
    "v = mdp_policy_evaluation(R, P, random_policy, gamma=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to evaluate the value function $v_{\\pi}$ of a given policy $\\pi$ with respect to an existing MDP.\n",
    "Once we have $v_{\\pi}$ we can use it to improve or policy by acting greedily w.r.t $v_{\\pi}$, i.e. we choose the action which leads to a state with maximal value function :\n",
    "$$ \\pi'(s) = arg\\max_{a∈A}(R^a_{s}+\\gamma \\sum_{s'}P^a_{ss'}v_{\\pi}(s'))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2034,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(v,R,P,gamma):\n",
    "    eps = np.finfo(np.float64).resolution\n",
    "    n_actions = P.shape[0]\n",
    "    n_states = P.shape[1]\n",
    "    new_policy = np.zeros_like(R)\n",
    "    Q = np.zeros_like(R)\n",
    "    \n",
    "    for s in range(1,n_states-1):\n",
    "        for a in range(n_actions):           \n",
    "            Q[a,s] = R[a,s] + gamma * np.sum(P[a,s,:] * v)\n",
    "    for s in range(1,n_states-1):\n",
    "        best_actions = np.where(np.abs(Q[:,s]-np.max(Q[:,s]))<=eps)[0]\n",
    "        n_best = len(best_actions)\n",
    "        for a in best_actions:\n",
    "            new_policy[a,s]= 1./n_best\n",
    "    return new_policy,Q\n",
    "\n",
    "def mdp_policy_iteration(R, P, epsilon= 10**-7, n_iter_max=200, gamma=1, verbose=False):\n",
    "    old_policy = make_random_policy(n_states, n_actions)\n",
    "\n",
    "    policy_stable = False\n",
    "    n_iter=0\n",
    "    while not policy_stable:\n",
    "        n_iter+=1\n",
    "        new_v = mdp_policy_evaluation(R, P, old_policy, epsilon= epsilon, n_iter_max=n_iter_max, gamma=gamma)\n",
    "        new_policy,Q = greedy(new_v,R,P,gamma)\n",
    "        policy_stable = np.array_equal(old_policy, new_policy)\n",
    "        old_policy = new_policy\n",
    "    if verbose:\n",
    "        print(\"Policy iteration did converge after %d steps\"%(n_iter))\n",
    "    return new_policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2030,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration did converge after 2 steps\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = mdp_policy_iteration(R, P, gamma=1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2031,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,\n",
       "        6.,  7.,  8.,  9., 10.,  0.])"
      ]
     },
     "execution_count": 2031,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_policy_evaluation(R,P,optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use directly a serie of value functions $v_1\\rightarrow v_2 \\dots \\rightarrow v_k \\rightarrow v_{k+1}$ without explicitly using a policy :\n",
    "$$ v_{k+1}(s) = \\max_{a∈A}(R^a_{s}+\\gamma \\sum_{s'}P^a_{ss'}v_k(s'))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2032,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_value_iteration(R, P, epsilon= 10**-7, n_iter_max=200, gamma=1, verbose=False):\n",
    "    V_k = np.zeros_like(R[0])\n",
    "    delta = epsilon+1\n",
    "    n_iter=1\n",
    "    while delta>epsilon and n_iter<=n_iter_max:\n",
    "        \n",
    "        V_k_1 = np.max(R + gamma * P @ V_k,axis=0)\n",
    "        delta = max(epsilon, np.max(np.abs(V_k_1-V_k)))\n",
    "        V_k = V_k_1\n",
    "        n_iter+=1\n",
    "    return V_k_1\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2033,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,\n",
       "        6.,  7.,  8.,  9., 10.,  0.])"
      ]
     },
     "execution_count": 2033,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_policy_iteration(R, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challengeIA",
   "language": "python",
   "name": "challengeia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
